---
title: "R Notebook"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

```{r include=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE)
```

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(httr)
library(stringdist)
library(rvest)    
library(stringr)
library(tidytext)
library(igraph)
library(ggraph)
data(stop_words)

US.table <- read_csv("Data/US_last_meals.csv")
```

```{r include=FALSE}
theme_custom <- function() {
  theme_gray() +
    theme(
      panel.grid.minor.y = element_line(color = NA),
      panel.grid.major.y = element_line(color = "gray95"),
      panel.grid.minor.x = element_line(color = NA),
      panel.grid.major.x = element_line(color = "gray95"),
      panel.background = element_rect(fill = NA),
      plot.background = element_rect(
        fill = NA,
        color = "gray95",
        size = 10
      ),
      plot.margin = unit(c(1, 1, 1, 1), "cm"),
      axis.title = element_text(color = "gray30"),
      axis.ticks = element_line(color = NA),
      strip.background = element_rect(fill = "gray95"),
      strip.text = element_text(
        color = "gray30",
        size = 11,
        face = "bold"
      ),
      plot.title = element_text(color = "gray30",
                                face = "bold"),
      plot.subtitle = element_text(size = 10,
                                   color = "gray30"),
      text = element_text(family = "Helvetica")
    )
}

theme_set(theme_custom())
```



Text
```{r}
parsed.words <- US.table %>%
  select(Name, Requested.Meal) %>% 
  unnest_tokens(output = word, input = Requested.Meal) %>% 
  anti_join(stop_words, by = "word")
```

```{r echo=FALSE}
# top words
parsed.words %>% 
  group_by(word) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) %>% 
  top_n(n = 15, wt = n) %>% 
  ggplot(aes(x = reorder(word, n), y = n, fill = n)) +
  geom_col() +
  scale_fill_gradient(low = "#0b2919", high = "#2b7551") +
  labs(title = "Most common words within last meal requests",
       x = "",
       y = "Count") +
  coord_flip() +
  theme(legend.position = "none")
```


Foodbase

Download foodbase from here: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6827550/

Pull out the ingredients which will act as comparison for our foods


```{r}
food.words <- read_csv("Data/food_words.csv") %>%
  pull() %>%
  str_to_lower()

# remove condiments to the food words list
excl.words <- c("meal", "food", "snack", "drink", "double", "ketchup", "cups",
                "mustard", "mayonnaise", "mayo", "sauce", "sour cream",
                "pepper", "ranch", "ranch dressing", "meat", "butter")
food.words <- food.words[!(food.words %in% excl.words)]

parsed.words %>% 
  group_by(word) %>%
  summarize(n = n()) %>% 
  arrange(desc(n)) %>% 
  mutate(match = word %in% food.words) %>%
  head()
```


```{r echo=FALSE}
# top food words
parsed.words %>% 
  filter(word %in% food.words) %>% 
  group_by(word) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) %>% 
  top_n(n = 15, wt = n) %>% 
  ggplot(aes(x = reorder(word, n), y = n, fill = n)) +
  geom_col() +
  scale_fill_gradient(low = "#0b2919", high = "#2b7551") +
  labs(title = "Most common food for last meal requests",
       x = "",
       y = "Count") +
  coord_flip() +
  theme(legend.position = "none")
```

# repeat with ngrams

```{r}
# tokenize the words skipping ngrams
#   this allows multiple words per token so we include
#   food like "fried chicken", not just "chicken"
parsed.ngrams <- US.table %>%
  select(Name, Requested.Meal) %>% 
  unnest_tokens(output = word, input = Requested.Meal, token = "skip_ngrams") %>% 
  anti_join(stop_words, by = "word") %>% 
  filter(word %in% food.words)
```

```{r echo=FALSE}
parsed.ngrams %>% 
  group_by(word) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) %>%
  top_n(n = 15, wt = n) %>% 
  ggplot(aes(x = reorder(word, n), y = n, fill = n)) +
  geom_col() +
  scale_fill_gradient(low = "#0b2919", high = "#2b7551") +
  labs(title = "Most common food for last meal requests",
       x = "Word",
       y = "Count") +
  coord_flip() +
  theme(legend.position = "none")
```

The issue now is that we're double counting some items. Ice, cream, and ice cream are the most obvious. We can remove this duplicates on an inmate basis by excluding words that exist in longer strings. E.g. exclude "ice" if there is another string like "ice cream" but keep "ice cream."

```{r}
# split the $word with more than one word into a list
#  then split the dataframe by $Name
# remove unrelated words
name.groups <- parsed.ngrams %>% 
  mutate(word = str_split(word, pattern = " ")) %>% 
  group_by(Name) %>% 
  group_split()

# check to see if the $word is contained within 
#  another $word for that $Name
deduped.ngrams <- lapply(name.groups, function(group) {
  
  # first remove $words that are more than two
  #  individual words (e.g. "chocolate ice cream") b/c
  #  these will be captured in "ice cream"
  group <- group %>%
    rowwise() %>%
    filter(length(word) <= 2) %>% 
    ungroup()
  
  # if only one unique word then return that one word
  if (length(unique(group$word)) == 1) {
    non.duplicates <- group$word[1]
  } else{
    # for groups that have more than one row check to 
    #   see if a word is contained in another row
    duplicate.bool <-
      sapply(1:length(group$word), function(i) {
        x <- group$word[i]
        lst <- group$word
        lst <- lst[!(lst %in% x)]
        word.in.list <- sapply(lst, function(y) {
          x %in% y
        })
        return(sum(word.in.list) == 0)
      })
    
    non.duplicates <- group$word[duplicate.bool]
  }
  return(group %>% filter(word %in% non.duplicates))
}) %>% bind_rows()
rm(name.groups)

# unlist the word column
deduped.ngrams <- deduped.ngrams %>% 
  rowwise() %>% 
  mutate(word = paste0(word, collapse = " ")) %>% 
  ungroup()

# n removed rows
nrow(parsed.ngrams) - nrow(deduped.ngrams)
```


```{r echo=FALSE}
# total counts of items
deduped.ngrams %>% 
  group_by(word) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) %>%
  top_n(n = 15, wt = n) %>% 
  ggplot(aes(x = reorder(word, n), y = n, fill = n)) +
  geom_col() +
  scale_fill_gradient(low = "#0b2919", high = "#2b7551") +
  labs(title = "Top 15 most common last meal requests",
       subtitle = "Data from 130 U.S. inmates since 1927",
       x = "",
       y = "Count") +
  coord_flip() +
  theme(legend.position = "none")
```


# check to see if there are any similar foods that are duplicates and should be edited
```{r}
# replace words that end in "s" if there
#  is an similar word in the dataset that doesn't
#  end in "s"
deduped.ngrams <- deduped.ngrams %>%
  mutate(S.word = if_else(substr(word,
                                 start = nchar(word),
                                 stop = nchar(word)) == "s",
                          substr(word,
                                 start = 0,
                                 stop = nchar(word)-1),
                          "NA"),
         word = if_else(S.word %in% word,
                        S.word,
                        word)) %>% 
  select(-S.word)

get_top_matches <- function(current.word, words.to.match, n = 5){
  # function returns that top n matches of the current.name
  #   within the names.to.match list via fuzzy string matching
  
  scores <- stringsim(current.word, words.to.match, method = "osa")
  words.to.match[rev(order(scores))][1:(n + 1)]
}

# test the function
get_top_matches(deduped.ngrams$word[1], unique(deduped.ngrams$word))

# apply the function across the entire list to generate a data.frame
#  containing the current.name and it's top 5 best matches
lapply(deduped.ngrams$word,
                      get_top_matches,
                      words.to.match = unique(deduped.ngrams$word)) %>% 
  unlist() %>% 
  matrix(ncol = 6, byrow = TRUE) %>% 
  as_tibble() %>% 
  setNames(c("Current.word", paste0("Match.", 1:5))) %>% 
  View
```

```{r echo=FALSE}
# total counts of items
deduped.ngrams %>% 
  group_by(word) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) %>%
  top_n(n = 15, wt = n) %>% 
  ggplot(aes(x = reorder(word, n), y = n, fill = n)) +
  geom_col() +
  scale_fill_gradient(low = "#0b2919", high = "#2b7551") +
  labs(title = "Top 15 most common items in last meal requests",
       subtitle = "Data from 130 U.S. inmates since 1927",
       x = "",
       y = "Count") +
  coord_flip() +
  theme(legend.position = "none")
```

```{r echo=FALSE}
# ggsave(filename = "Plots/Common_last_meals.svg",
#        plot = last_plot(),
#        device = "svg",
#        width = 8,
#        height = 7)
```

```{r echo=FALSE}
# histogram of number of different typles of food requested
deduped.ngrams %>% 
  group_by(Name) %>%
  summarize(n.foods = n()) %>%
  ggplot(aes(x = n.foods)) +
  geom_histogram(binwidth = 1, color = "white") +
  labs(title = "Breadth of food types within last meal requests",
       x = "Number of foods requested",
       y = "")
```

```{r echo=FALSE}
# frequency of food over time
deduped.ngrams %>%
  left_join(y = US.table[, c("Name", "Year")]) %>% 
  mutate(Decade = floor(Year / 10) * 10) %>% 
  group_by(Decade, word) %>%
  summarize(n.foods = n()) %>%
  filter(n.foods > 3) %>% 
  ggplot(aes(x = word, y = n.foods)) +
  geom_col() +
  facet_wrap(~Decade, nrow = 1) +
  labs(title = "Frequency of food types over time",
       subtitle = "Must contain at least 4 occurences",
       x = "",
       y = "") +
  coord_flip()
```

# correlation

Explain process



```{r}
cosine_matrix <- function(tokenized_data, lower = 0, upper = 1, filt = 0) {
  # ruthlessly stolen from https://www.markhw.com/blog/word-similarity-graphs
  
  if (!all(c("word", "Name") %in% names(tokenized_data))) {
    stop("tokenized_data must contain variables named word and id")
  }
  
  if (lower < 0 | lower > 1 | upper < 0 | upper > 1 | filt < 0 | filt > 1) {
    stop("lower, upper, and filt must be 0 <= x <= 1")
  }
  
  docs <- length(unique(tokenized_data$Name))
  
  out <- tokenized_data %>%
    count(Name, word) %>%
    group_by(word) %>%
    mutate(n_docs = n()) %>%
    ungroup() %>%
    filter(n_docs < (docs * upper) & n_docs > (docs * lower)) %>%
    select(-n_docs) %>%
    mutate(n = 1) %>%
    spread(word, n, fill = 0) %>%
    select(-Name) %>%
    as.matrix() %>%
    lsa::cosine()
  
  filt <- quantile(out[lower.tri(out)], filt)
  out[out < filt] <- diag(out) <- 0
  out <- out[rowSums(out) != 0, colSums(out) != 0]
  
  return(out)
}

cos_mat <- cosine_matrix(deduped.ngrams, lower = .035,
                         upper = .90, filt = .75)


```

There are two clear groupings here. In the right section, "home-style" food such as mashed potatoes, gravy, tea, peas, and rice all naturally go together. In the center of the left section, hamburger, onion rings, fried chicken, and steak also are naturally grouped. Intiutively, cheese is stuck right in the middle of these two groups. I interpret this as it's the great equalizer, [almost everyone loves cheese](link).

```{r}
set.seed(44)
graph_from_adjacency_matrix(cos_mat, 
                            mode = "undirected", 
                            weighted = TRUE) %>%
  ggraph(layout = 'nicely') +
  geom_edge_link(aes(alpha = weight),
                 show.legend = FALSE,
                 color = "#2b7551") + 
  geom_node_label(aes(label = name),
                  label.size = 0.1,
                  size = 2,
                  color = "#0b2919")
```

```{r echo=FALSE}
# ggsave(filename = "Plots/last_meals_graph.svg",
#        plot = last_plot(),
#        device = "svg",
#        width = 8,
#        height = 7)
```





## Estimate calories per meal


```{r}
library(NutrienTrackeR)

food_composition_data$USDA %>% 
  as_tibble() %>% 
  filter(str_detect(food_name, 'ice cream')) %>% 
  View

findFoodName(keywords = c("fries"), food_database = "USDA")

```

https://github.com/hadley/usdanutrients



